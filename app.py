import gc
import logging
import os
import tempfile
import time
from datetime import datetime
from functools import lru_cache
from retrying import retry

import easyocr
import fasttext
import gradio as gr
import moviepy as mp
import torch
from pyannote.audio import Pipeline
from transformers.pipelines import pipeline
from TTS.api import TTS

import yt_dlp  # –î–ª—è YouTube
from pydub import AudioSegment  # –î–ª—è VAD
import pysrt  # –î–ª—è SRT (pip install pysrt)


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.DEBUG,  # –£–ª—É—á—à–µ–Ω–æ: debug –¥–ª—è –¥–µ—Ç–∞–ª–µ–π
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

try:
    import whisperx
except ImportError:
    logger.error("whisperx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install git+https://github.com/m-bain/whisperX.git")
    whisper_model = None

# ==================== –ù–ê–°–¢–†–û–ô–ö–ê NLLB ====================
NLLB_LANG_MAP = {
    "en": "eng_Latn",
    "ru": "rus_Cyrl",
    "fr": "fra_Latn",
    "de": "deu_Latn",
    "es": "spa_Latn",
    "zh": "zho_Hans",
    "ja": "jpn_Jpan",
    "ar": "arb_Arab",
    "pt": "por_Latn",
    "it": "ita_Latn",
    "ko": "kor_Hang",
    "hi": "hin_Deva",
    "nl": "nld_Latn",  # –î–æ–±–∞–≤–ª–µ–Ω–æ: –≥–æ–ª–ª–∞–Ω–¥—Å–∫–∏–π
    # –î–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
}

# ==================== –ù–ê–°–¢–†–û–ô–ö–ê –ü–£–¢–ï–ô ====================
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
MODELS_DIR = os.path.join(PROJECT_ROOT, "models")

for subdir in ["whisper", "tts", "translation", "ocr", "huggingface", "fasttext"]:
    os.makedirs(os.path.join(MODELS_DIR, subdir), exist_ok=True)

os.environ["HF_HUB_CACHE"] = os.path.join(MODELS_DIR, "huggingface")
os.environ["TRANSFORMERS_CACHE"] = os.path.join(MODELS_DIR, "huggingface")
os.environ["HF_HOME"] = os.path.join(MODELS_DIR, "huggingface")
os.environ["COQUI_TTS_CACHE"] = os.path.join(MODELS_DIR, "tts")

FASTTEXT_MODEL_PATH = os.path.join(MODELS_DIR, "fasttext", "lid.176.bin")

logger.info(f"–ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤: {MODELS_DIR}")

# –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
device = "cuda" if torch.cuda.is_available() else "cpu"
logger.info(f"Using device: {device}")

# –ó–∞–≥—Ä—É–∑–∫–∞ fastText –º–æ–¥–µ–ª–∏ —Å retry
@retry(stop_max_attempt_number=3, wait_fixed=2000)
def download_fasttext():
    if not os.path.exists(FASTTEXT_MODEL_PATH):
        logger.info("–ú–æ–¥–µ–ª—å fastText lid.176.bin –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–∫–∞—á–∏–≤–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏...")
        import urllib.request
        url = "https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin"
        urllib.request.urlretrieve(url, FASTTEXT_MODEL_PATH)
        logger.info("fastText –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

fasttext_model = None
try:
    download_fasttext()
    fasttext_model = fasttext.load_model(FASTTEXT_MODEL_PATH)
    logger.info("fastText –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –¥–ª—è language detection.")
except Exception as e:
    logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ fastText –º–æ–¥–µ–ª–∏: {e}")
    fasttext_model = None

# –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
WHISPER_MODELS = {
    "large-v3": "large-v3",
    "large-v3-turbo": "large-v3-turbo",
    "medium": "medium",
    "small": "small",
    "distil-large-v3": "Systran/faster-whisper-distil-large-v3",
}

TTS_MODELS = {
    "your_tts (multilingual)": "tts_models/multilingual/multi-dataset/your_tts",
    "en/tacotron2-DDC": "tts_models/en/ljspeech/tacotron2-DDC",
    "ru/vits": "tts_models/ru/multi-dataset/vits",
    "en/vits": "tts_models/en/ljspeech/vits",
    "en/vits-persian": "tts_models/en/vctk/vits",  # –ï—â—ë –æ–¥–Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å
}

# –ö–∞—Ä—Ç–∞ —Å–ø–∏–∫–µ—Ä–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ —è–∑—ã–∫–æ–≤
TTS_SPEAKERS = {
    "your_tts (multilingual)": {
        "en": "LJSpeech",
        "ru": "Russian Female",
        "fr": "French Female", 
        "de": "German Female",
        "es": "Spanish Female",
        "it": "Italian Female",
        "pt": "Portuguese Female",
        "default": "LJSpeech"
    },
    "ru/vits": {
        "ru": "Russian Female",
        "default": "Russian Female"
    },
    "en/vits": {
        "en": "p225",  # speaker_id –∏–∑ VCTK
        "default": "p225"
    },
    "en/vits-persian": {
        "en": "p225",
        "default": "p225"
    }
}

TRANSLATION_MODELS = [
    "Helsinki-NLP/opus-mt-mul-en",
    "Helsinki-NLP/opus-mt-tc-big-mul-en",
    "facebook/nllb-200-distilled-600M",
    "facebook/nllb-200-distilled-1.3B",
]

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
whisper_model = None
tts_model = None
current_whisper_name = None
current_tts_name = None
model_status_text = "–ú–æ–¥–µ–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã"

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OCR —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º —Å–ø–∏—Å–∫–æ–º —è–∑—ã–∫–æ–≤
SUPPORTED_OCR_LANGUAGES = ['en', 'ru', 'fr', 'de', 'es', 'ja', 'ch_sim']  # –î–æ–±–∞–≤–ª–µ–Ω –∫–∏—Ç–∞–π—Å–∫–∏–π
try:
    ocr_reader = easyocr.Reader(
        SUPPORTED_OCR_LANGUAGES,
        download_enabled=True,
        model_storage_directory=os.path.join(MODELS_DIR, "ocr"),
        user_network_directory=os.path.join(MODELS_DIR, "ocr"),
        gpu=device == "cuda"
    )
    logger.info(f"OCR –º–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è —è–∑—ã–∫–æ–≤: {SUPPORTED_OCR_LANGUAGES}")
except Exception as e:
    logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OCR: {e}")
    # Fallback –Ω–∞ en –∏ ru
    try:
        ocr_reader = easyocr.Reader(
            ['en', 'ru'],
            download_enabled=True,
            model_storage_directory=os.path.join(MODELS_DIR, "ocr"),
            user_network_directory=os.path.join(MODELS_DIR, "ocr"),
            gpu=device == "cuda"
        )
        logger.info("OCR –º–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è en –∏ ru")
    except Exception as e2:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ OCR: {e2}")
        ocr_reader = None

@lru_cache(maxsize=32)
def get_translator(model_name):
    try:
        return pipeline(
            "translation",
            model=model_name,
            device=device if device == "cuda" else -1,
            max_length=2048  # –£–≤–µ–ª–∏—á–µ–Ω–æ
        )
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–≤–æ–¥–∞ {model_name}: {e}")
        raise

def log(msg):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] {msg}")
    logger.info(msg)

def timed_step(step_name, func, *args, **kwargs):
    start = time.time()
    log(f"–ù–∞—á–∞–ª–æ: {step_name}")
    try:
        result = func(*args, **kwargs)
        elapsed = time.time() - start
        log(f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ: {step_name} ‚Üí {elapsed:.2f} —Å–µ–∫")
        return result, elapsed
    except Exception as e:
        elapsed = time.time() - start
        log(f"–û—à–∏–±–∫–∞ –≤ {step_name}: {e} ‚Üí {elapsed:.2f} —Å–µ–∫")
        raise

def detect_language_fasttext(text):
    if not text.strip() or fasttext_model is None:
        return "unknown", 0.0
    try:
        prediction = fasttext_model.predict([text], k=1)
        lang_label = prediction[0][0][0]
        prob = prediction[1][0][0]
        lang_code = lang_label.replace('__label__', '')
        return lang_code, prob
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —è–∑—ã–∫–∞: {e}")
        return "unknown", 0.0

def load_whisper(model_key):
    global whisper_model, current_whisper_name
    if current_whisper_name == model_key and whisper_model is not None:
        return f"WhisperX —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω: {model_key}"
    
    try:
        device_type = "cuda" if torch.cuda.is_available() else "cpu"
        compute_type = "float16" if device_type == "cuda" else "int8"
        
        whisper_model = whisperx.load_model(
            model_key,                  # "large-v3", "medium", "small", "large-v2" –∏ —Ç.–¥.
            device=device_type,
            compute_type=compute_type,
            download_root=os.path.join(MODELS_DIR, "whisper")
        )
        current_whisper_name = model_key
        logger.info(f"WhisperX –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_key} –Ω–∞ {device_type}")
        return f"WhisperX –∑–∞–≥—Ä—É–∂–µ–Ω: {model_key}"
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ WhisperX {model_key}: {e}")
        return f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ WhisperX: {str(e)}"

def load_tts(model_key):
    global tts_model, current_tts_name
    if current_tts_name == model_key and tts_model is not None:
        return f"TTS —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω: {model_key}"
    
    try:
        model_name = TTS_MODELS[model_key]
        tts_model = TTS(model_name=model_name, progress_bar=True).to(device)
        current_tts_name = model_key
        return f"TTS –∑–∞–≥—Ä—É–∂–µ–Ω: {model_key}"
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ TTS {model_key}: {e}")
        return f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ TTS: {str(e)}"


def preprocess_audio(audio_path, use_vad=False, use_uvr=False):
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ: VAD (—É–¥–∞–ª–µ–Ω–∏–µ —Ç–∏—à–∏–Ω—ã) + UVR (—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤–æ–∫–∞–ª–∞)"""
    if use_uvr:
        # UVR: –ò—Å–ø–æ–ª—å–∑—É–µ–º ultimatevocalremover (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)
        try:
            from uvr import uvr
            vocal_path, _ = uvr(audio_path)  # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–æ–∫–∞–ª –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª
            audio_path = vocal_path
            logger.info("UVR: –í–æ–∫–∞–ª –æ—Ç–¥–µ–ª—ë–Ω")
        except ImportError:
            logger.warning("UVR –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
    
    if use_vad:
        # Silero VAD: –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad')
        (get_speech_timestamps, _, read_audio, _, _) = utils
        
        sampling_rate = 16000  # Silero –æ–∂–∏–¥–∞–µ—Ç 16kHz
        wav = read_audio(audio_path, sampling_rate=sampling_rate)
        speech_timestamps = get_speech_timestamps(wav, model, threshold=0.6)
        
        if not speech_timestamps:
            return audio_path  # –ù–µ—Ç —Ä–µ—á–∏
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ speech —Å–µ–≥–º–µ–Ω—Ç—ã
        audio = AudioSegment.from_wav(audio_path)
        speech_audio = AudioSegment.silent(duration=0)
        for ts in speech_timestamps:
            start_ms = ts['start'] * (1000 / sampling_rate)
            end_ms = ts['end'] * (1000 / sampling_rate)
            speech_audio += audio[start_ms:end_ms]
        
        vad_path = tempfile.mktemp(suffix=".wav")
        speech_audio.export(vad_path, format="wav")
        audio_path = vad_path
        logger.info("VAD: –¢–∏—à–∏–Ω–∞ —É–¥–∞–ª–µ–Ω–∞")
    
    return audio_path

def download_youtube(url):
    """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ YouTube –≤–∏–¥–µ–æ/–∞—É–¥–∏–æ"""
    if not url:
        return None
    try:
        ydl_opts = {
            'format': 'bestaudio/best',
            'outtmpl': tempfile.mktemp(suffix=".mp4"),
            'quiet': True,
        }
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=True)
            return info['requested_downloads'][0]['filepath']
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ YouTube: {e}")
        return None

def translate_segments(segments, source_lang, target_lang, model_name):
    """–ü–µ—Ä–µ–≤–æ–¥ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º"""
    translated_segments = []
    for seg in segments:
        if seg.text.strip():
            trans_text = translate_text(seg.text.strip(), source_lang, target_lang, model_name)
            translated_segments.append({
                'start': seg.start,
                'end': seg.end,
                'text': trans_text
            })
    return translated_segments

def generate_srt(segments, file_path):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è SRT"""
    subs = pysrt.SubRipFile()
    for i, seg in enumerate(segments, 1):
        start = pysrt.SubRipTime(seconds=seg['start'])
        end = pysrt.SubRipTime(seconds=seg['end'])
        subs.append(pysrt.SubRipItem(index=i, start=start, end=end, text=seg['text']))
    subs.save(file_path, encoding='utf-8')
    return file_path

def load_selected_models(whisper_model_key, tts_model_key):
    global model_status_text
    status = []
    
    try:
        if whisper_model_key:
            status.append(load_whisper(whisper_model_key))
        if tts_model_key:
            status.append(load_tts(tts_model_key))
        
        model_status_text = "\n".join(status) if status else "–ú–æ–¥–µ–ª–∏ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–ª–∏ –Ω–µ –≤—ã–±—Ä–∞–Ω—ã"
    except Exception as e:
        model_status_text = f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {str(e)}"
    
    return model_status_text

def cleanup_memory():
    gc.collect()
    if device == "cuda":
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()

def extract_audio_from_video(video_path):
    try:
        video = mp.VideoFileClip(video_path)
        audio_path = tempfile.mktemp(suffix=".wav")
        video.audio.write_audiofile(audio_path, logger=None)
        video.close()
        return audio_path
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞—É–¥–∏–æ: {e}")
        raise

def transcribe_audio(audio_path, source_lang="auto"):
    if whisper_model is None:
        raise ValueError("WhisperX –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    try:
        # 1. –ë–∞–∑–æ–≤–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
        transcribe_result = whisper_model.transcribe(
            audio_path,
            language=None if source_lang.lower() == "auto" else source_lang,
            batch_size=16,               # –ø–æ–¥–±–µ—Ä–∏ –ø–æ–¥ —Å–≤–æ—é –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—É (8‚Äì32)
            chunk_size=30,               # —Å–µ–∫ ‚Äî –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            print_progress=True
        )
        
        detected_lang = transcribe_result.get("language", "unknown")
        detected_prob = transcribe_result.get("language_probability", 0.0)
        
        # 2. –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ (word-level timestamps) ‚Äî –æ—á–µ–Ω—å –≤–∞–∂–Ω–æ –¥–ª—è SRT
        progress(0.4, desc="–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ (alignment)...")
        align_model, metadata = whisperx.load_align_model(
            language_code=detected_lang,
            device=device
        )
        
        aligned_result = whisperx.align(
            transcribe_result["segments"],
            align_model,
            metadata,
            audio_path,
            device,
            return_char_alignments=False  # word-level –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
        )
        
        segments = aligned_result["segments"]  # —É–∂–µ —Å 'start', 'end', 'text', 'words'
        
        # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
        full_text = " ".join(seg["text"] for seg in segments)
        
        # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º –∫–æ–¥–æ–º –≤–æ–∑–≤—Ä–∞—â–∞–µ–º list —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        # –ö–∞–∂–¥—ã–π —Å–µ–≥–º–µ–Ω—Ç –∏–º–µ–µ—Ç 'start', 'end', 'text'
        compatible_segments = [
            {
                "start": seg["start"],
                "end": seg["end"],
                "text": seg["text"].strip()
            }
            for seg in segments if seg.get("text", "").strip()
        ]
        
        return full_text, detected_lang, detected_prob, compatible_segments
    
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ WhisperX: {e}", exc_info=True)
        raise

def ocr_image(image_path, source_lang="en"):
    if ocr_reader is None:
        raise ValueError("OCR –º–æ–¥–µ–ª—å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    
    try:
        if source_lang == "auto":
            lang_list = SUPPORTED_OCR_LANGUAGES
        else:
            lang_list = [source_lang] if source_lang in SUPPORTED_OCR_LANGUAGES else ['en']
            if source_lang not in SUPPORTED_OCR_LANGUAGES:
                logger.warning(f"–Ø–∑—ã–∫ {source_lang} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è OCR. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 'en'.")
        
        result = ocr_reader.readtext(
            image_path, 
            detail=0, 
            paragraph=True, 
            lang_list=lang_list
        )
        return " ".join(result)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ OCR: {e}")
        raise

def translate_text(text, source_lang, target_lang, model_name):
    if not text.strip():
        return ""
    
    try:
        if source_lang == target_lang:
            return text
        
        translator = get_translator(model_name)
        
        if "nllb" in model_name.lower():
            src_code = NLLB_LANG_MAP.get(source_lang, f"{source_lang}_Latn")
            tgt_code = NLLB_LANG_MAP.get(target_lang, f"{target_lang}_Latn")
            result = translator(text, src_lang=src_code, tgt_lang=tgt_code, max_length=2048)  # –£–≤–µ–ª–∏—á–µ–Ω–æ
            return result[0]['translation_text']
        else:
            # Pivot —á–µ—Ä–µ–∑ en
            if source_lang.lower() != "en":
                try:
                    direct_model = f"Helsinki-NLP/opus-mt-{source_lang}-en"
                    direct_translator = get_translator(direct_model)
                    en_text = direct_translator(text)[0]['translation_text']
                except:
                    en_text = translator(text, src_lang=source_lang)[0]['translation_text']
            else:
                en_text = text
            
            if target_lang.lower() == "en":
                return en_text
            
            try:
                tgt_model = f"Helsinki-NLP/opus-mt-en-{target_lang}"
                tgt_translator = get_translator(tgt_model)
                return tgt_translator(en_text)[0]['translation_text']
            except:
                return en_text  # Fallback
                
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
        return f"[–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {str(e)}]"

def diarize_audio(audio_path, hf_token):
    if not hf_token or not audio_path:
        return []
    
    logger.info("–ó–∞–ø—É—Å–∫ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏...")
    try:
        pipe = Pipeline.from_pretrained(
            "pyannote/speaker-diarization-3.1", 
            use_auth_token=hf_token
        ).to(torch.device(device))
        
        diarization = pipe(audio_path)
        segments = []
        
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            segments.append({
                "start": turn.start, 
                "end": turn.end, 
                "speaker": speaker
            })
        
        return segments
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
        return []

def merge_transcription_and_diarization(whisper_segments, diar_segments):
    if not diar_segments:
        return "\n".join([f"{s.text}" for s in whisper_segments])
    
    final = []
    for w_seg in whisper_segments:
        start, end = w_seg.start, w_seg.end
        speakers = {}
        
        for d in diar_segments:
            o_start = max(start, d["start"])
            o_end = min(end, d["end"])
            overlap = max(0, o_end - o_start)
            
            if overlap > 0:
                speakers[d["speaker"]] = speakers.get(d["speaker"], 0) + overlap
        
        best = max(speakers, key=speakers.get) if speakers else "Unknown"
        final.append(f"[{best}] ({start:.1f}-{end:.1f}): {w_seg.text}")
    
    return "\n".join(final)

def text_to_speech(text, target_lang="en", ref_audio=None, tts_model_name=None):
    if tts_model is None or not text.strip():
        return None
    
    output_path = tempfile.mktemp(suffix=".wav")
    model_name = tts_model_name or current_tts_name or "unknown"
    
    try:
        speaker = None
        if model_name in TTS_SPEAKERS:
            speaker = TTS_SPEAKERS[model_name].get(target_lang, TTS_SPEAKERS[model_name]["default"])
        
        model_str = str(tts_model).lower()
        
        if "xtts" in model_str:
            if ref_audio and os.path.exists(ref_audio):
                tts_model.tts_to_file(text=text, file_path=output_path, speaker_wav=ref_audio, language=target_lang)
            else:
                tts_model.tts_to_file(text=text, file_path=output_path, language=target_lang)
        
        elif "your_tts" in model_str:
            tts_model.tts_to_file(text=text, file_path=output_path, speaker=speaker, language=target_lang)  # –£–±—Ä–∞–Ω–æ –∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        
        elif "vits" in model_str:
            tts_model.tts_to_file(text=text, file_path=output_path, speaker=speaker)  # –î–ª—è VITS –±–µ–∑ —è–∑—ã–∫–∞, –µ—Å–ª–∏ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        
        else:
            # –î–ª—è Tacotron –∏ –¥—Ä—É–≥–∏—Ö
            try:
                tts_model.tts_to_file(text=text, file_path=output_path, language=target_lang)
            except:
                tts_model.tts_to_file(text=text, file_path=output_path)  # Fallback –±–µ–∑ —è–∑—ã–∫–∞
        
        return output_path
        
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ TTS –¥–ª—è –º–æ–¥–µ–ª–∏ {model_name}: {e}")
        return None

def auto_detect_media_type(file_path):
    if not file_path:
        return None
    ext = os.path.splitext(file_path)[1].lower()
    if ext in ['.mp3', '.wav', '.ogg']:
        return "Audio"
    elif ext in ['.mp4', '.avi', '.mov']:
        return "Video"
    elif ext in ['.jpg', '.png', '.bmp']:
        return "Image"
    return None

def validate_inputs(input_type, file, input_text, media_type):
    errors = []
    
    if input_type == "File":
        if not file:
            errors.append("–§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω")
        elif media_type not in ["Audio", "Video", "Image"]:
            errors.append("–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø –º–µ–¥–∏–∞")
    elif input_type == "Text":
        if not input_text.strip():
            errors.append("–¢–µ–∫—Å—Ç –Ω–µ –≤–≤–µ–¥—ë–Ω")
    
    return errors

def process_media(
    progress=gr.Progress(),
    input_type=None, file=None, input_text="", youtube_url="",
    media_type=None, source_lang="auto", target_lang="ru",
    do_transcribe=True, do_translate=True, do_tts=True,
    whisper_model_key="large-v3", tts_model_key="your_tts (multilingual)",
    translation_model="facebook/nllb-200-distilled-600M",
    do_diarization=False, hf_token="",
    ref_audio=None, use_vad=False, use_uvr=False,
    use_direct_translate=False, output_srt=True
):
    progress(0, desc="–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
    start_total = time.time()
    timings = []
    warnings = []
    result = ""

    # ‚îÄ‚îÄ‚îÄ –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ YouTube ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    if youtube_url.strip():
        progress(0.05, desc="–°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Å YouTube...")
        file = download_youtube(youtube_url)
        if not file:
            return "‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è YouTube", None, None, None, None
        media_type = "Video"

    if not media_type and file:
        media_type = auto_detect_media_type(file) or "Unknown"

    if do_transcribe and whisper_model is None:
        load_whisper(whisper_model_key)

    if do_transcribe and whisper_model is None:
        warnings.append("WhisperX –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")

    if warnings:
        return "\n".join(warnings) + f"\n\n{model_status_text}", None, None, None, None

    # ‚îÄ‚îÄ‚îÄ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∞—É–¥–∏–æ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    audio_p = None
    text = ""
    detected_lang = source_lang if source_lang != "auto" else "unknown"
    detected_prob = 0.0
    whisper_segments = []   # —Ç–µ–ø–µ—Ä—å –æ—Ç WhisperX

    if do_transcribe and media_type in ("Audio", "Video"):
        progress(0.2, desc="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∞—É–¥–∏–æ...")
        
        if media_type == "Video":
            audio_p = extract_audio_from_video(file)
        else:
            audio_p = file

        if not audio_p or not os.path.exists(audio_p):
            return "‚ùå –ê—É–¥–∏–æ-—Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω", None, None, None, None

        audio_p = preprocess_audio(audio_p, use_vad=use_vad, use_uvr=use_uvr)

        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è + alignment —á–µ—Ä–µ–∑ WhisperX
        progress(0.35, desc="–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è + –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ (WhisperX)...")
        try:
            full_text, lang, prob, segments = timed_step(
                "WhisperX (transcribe + align)",
                transcribe_audio,
                audio_p,
                source_lang
            )[0]

            text = full_text
            detected_lang = lang
            detected_prob = prob
            whisper_segments = segments

            timings.append(("WhisperX —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è + alignment", timings[-1][1] if timings else 0))

        except Exception as e:
            text = f"[WhisperX –æ—à–∏–±–∫–∞: {str(e)}]"
            logger.error("WhisperX failed", exc_info=True)

    elif do_transcribe and media_type == "Image":
        # OCR –æ—Å—Ç–∞—ë—Ç—Å—è –∫–∞–∫ –±—ã–ª–æ
        text, t_ocr = timed_step("OCR", ocr_image, file, source_lang if source_lang != "auto" else "auto")
        if fasttext_model and text.strip():
            detected_lang, detected_prob = detect_language_fasttext(text)
        timings.append(("OCR + lang detect", t_ocr))

    else:
        text = input_text.strip() or "[–¢–µ–∫—Å—Ç –Ω–µ –≤–≤–µ–¥—ë–Ω]"
        timings.append(("–í–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞", 0))

    # ‚îÄ‚îÄ‚îÄ –ü–µ—Ä–µ–≤–æ–¥ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    translated_text = text
    translated_segments = []

    if do_translate and text.strip() and not text.startswith("["):
        progress(0.65, desc="–ü–µ—Ä–µ–≤–æ–¥...")
        src_lang = source_lang if source_lang != "auto" else detected_lang

        if whisper_segments:
            # –ü–µ—Ä–µ–≤–æ–¥–∏–º —Å–µ–≥–º–µ–Ω—Ç—ã ‚Üí —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–π–º–∏–Ω–≥–∏
            translated_segments, t_trans = timed_step(
                "–ü–µ—Ä–µ–≤–æ–¥ —Å–µ–≥–º–µ–Ω—Ç–æ–≤",
                translate_segments,
                whisper_segments,
                src_lang,
                target_lang,
                translation_model
            )
            translated_text = "\n".join(s["text"] for s in translated_segments)
        else:
            translated_text, t_trans = timed_step(
                "–ü–µ—Ä–µ–≤–æ–¥ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞",
                translate_text,
                text,
                src_lang,
                target_lang,
                translation_model
            )
        timings.append((f"–ü–µ—Ä–µ–≤–æ–¥ ‚Üí {target_lang}", t_trans))

    # ‚îÄ‚îÄ‚îÄ TTS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    tts_audio_path = None
    if do_tts and translated_text.strip() and not translated_text.startswith("["):
        progress(0.8, desc="–°–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏...")
        tts_lang = target_lang if do_translate else detected_lang
        tts_audio_path, t_tts = timed_step(
            f"TTS ({tts_lang})",
            text_to_speech,
            translated_text,
            tts_lang,
            ref_audio,
            tts_model_key
        )
        timings.append(("TTS", t_tts))

    # ‚îÄ‚îÄ‚îÄ –≠–∫—Å–ø–æ—Ä—Ç —Ñ–∞–π–ª–æ–≤ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    text_file_path = tempfile.mktemp(suffix=".txt") if translated_text.strip() else None
    if text_file_path:
        with open(text_file_path, "w", encoding="utf-8") as f:
            f.write(translated_text)

    srt_file_path = None
    if output_srt and (translated_segments or whisper_segments):
        srt_file_path = tempfile.mktemp(suffix=".srt")
        segments_for_srt = translated_segments if do_translate and translated_segments else whisper_segments
        try:
            generate_srt(segments_for_srt, srt_file_path)
        except Exception as e:
            logger.error(f"SRT generation failed: {e}")
            srt_file_path = None

    # ‚îÄ‚îÄ‚îÄ –†–µ–∑—É–ª—å—Ç–∞—Ç ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    progress(0.95, desc="–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞...")
    
    prob_str = f" ({detected_prob:.0%})" if detected_prob > 0 else ""
    result = f"**WhisperX** | –Ø–∑—ã–∫: {detected_lang}{prob_str}\n\n"
    if text:
        result += f"–û—Ä–∏–≥–∏–Ω–∞–ª:\n{text.strip()[:800]}...\n\n"
    if translated_text != text:
        result += f"–ü–µ—Ä–µ–≤–æ–¥ ({target_lang}):\n{translated_text.strip()[:800]}...\n\n"

    total_time = time.time() - start_total
    timings.append(("–í—Å–µ–≥–æ", total_time))

    result += "```\n–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:\n"
    for name, sec in timings:
        result += f"{name:.<40} {sec:>6.1f} —Å\n"
    result += "```"

    # –û—á–∏—Å—Ç–∫–∞
    if audio_p and audio_p != file and os.path.exists(audio_p):
        try:
            os.remove(audio_p)
        except:
            pass
    cleanup_memory()

    progress(1.0, desc="–ì–æ—Ç–æ–≤–æ!")
    return result, tts_audio_path, tts_audio_path, text_file_path, srt_file_path

# ==================== GRADIO –ò–ù–¢–ï–†–§–ï–ô–° ====================

css = """
    .result-textbox textarea {
        min-height: 220px;
        max-height: 65vh;
        overflow-y: auto !important;
        resize: vertical;
        font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
        font-size: 14px;
    }
    .warning {
        color: #ff6b00;
        font-weight: bold;
    }
    .success {
        color: #00aa00;
        font-weight: bold;
    }
    .error {
        color: #ff0000;
        font-weight: bold;
    }
    .info-box {
        padding: 10px;
        border-radius: 5px;
        background: #f0f8ff;
        border-left: 4px solid #4a90e2;
        margin: 10px 0;
    }
"""

with gr.Blocks(css=css, theme="soft") as demo:
    
    gr.Markdown("# üéØ Local Media Processor")
    gr.Markdown("–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è, OCR, –ø–µ—Ä–µ–≤–æ–¥ –∏ —Å–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏ –≤ –æ–¥–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏")
    
    # –°–µ–∫—Ü–∏—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
    with gr.Accordion("üîê –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏", open=False):
        hf_token_input = gr.Textbox(
            label="HuggingFace Token (–¥–ª—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ pyannote)", 
            type="password",
            placeholder="hf_...",
            info="–¢–æ–∫–µ–Ω –Ω—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ –¥–ª—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏"
        )
    
    # –°–µ–∫—Ü–∏—è –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–µ–π
    with gr.Accordion("ü§ñ –í—ã–±–æ—Ä –º–æ–¥–µ–ª–µ–π", open=True):
        with gr.Row():
            whisper_dropdown = gr.Dropdown(
                choices=list(WHISPER_MODELS.keys()), 
                label="–ú–æ–¥–µ–ª—å Whisper", 
                value="large-v3",
                info="–î–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∞—É–¥–∏–æ/–≤–∏–¥–µ–æ"
            )
            tts_dropdown = gr.Dropdown(
                choices=list(TTS_MODELS.keys()), 
                label="–ú–æ–¥–µ–ª—å TTS", 
                value="your_tts (multilingual)",
                info="–î–ª—è —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏"
            )
        
        translation_model_dropdown = gr.Dropdown(
            choices=TRANSLATION_MODELS,
            label="–ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–≤–æ–¥–∞",
            value="facebook/nllb-200-distilled-600M",
            info="NLLB –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –±–æ–ª—å—à–µ —è–∑—ã–∫–æ–≤"
        )
    
    # –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–æ–ª–æ—Å–∞
    with gr.Accordion("üé§ –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–æ–ª–æ—Å–∞", open=False):
        do_cloning = gr.Checkbox(
            label="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–æ–ª–æ—Å–∞", 
            value=False,
            info="–¢—Ä–µ–±—É–µ—Ç—Å—è —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–µ –∞—É–¥–∏–æ"
        )
        ref_audio_input = gr.Audio(
            label="–û–±—Ä–∞–∑–µ—Ü –≥–æ–ª–æ—Å–∞ (Reference Audio)", 
            type="filepath", 
            visible=False
        )
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
    load_models_btn = gr.Button("üîÑ –ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏", variant="primary")
    model_status = gr.Textbox(
        label="–°—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–µ–π", 
        interactive=False, 
        lines=3, 
        value=model_status_text
    )
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —è–∑—ã–∫–∞—Ö (–æ–±–Ω–æ–≤–ª–µ–Ω–æ)
    with gr.Accordion("‚ÑπÔ∏è –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —è–∑—ã–∫–∏", open=False):
        gr.Markdown("""
        ### –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è (Whisper):
        - –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ 100 —è–∑—ã–∫–æ–≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        
        ### OCR (EasyOCR):
        - –ê–Ω–≥–ª–∏–π—Å–∫–∏–π (en), –†—É—Å—Å–∫–∏–π (ru), –§—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π (fr)
        - –ù–µ–º–µ—Ü–∫–∏–π (de), –ò—Å–ø–∞–Ω—Å–∫–∏–π (es), –Ø–ø–æ–Ω—Å–∫–∏–π (ja), –ö–∏—Ç–∞–π—Å–∫–∏–π (ch_sim)
        
        ### –ü–µ—Ä–µ–≤–æ–¥ (NLLB):
        - –ë–æ–ª–µ–µ 200 —è–∑—ã–∫–æ–≤
        
        ### TTS (YourTTS –∏ –¥—Ä.):
        - –ê–Ω–≥–ª–∏–π—Å–∫–∏–π, –†—É—Å—Å–∫–∏–π, –§—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π, –ù–µ–º–µ—Ü–∫–∏–π
        - –ò—Å–ø–∞–Ω—Å–∫–∏–π, –ò—Ç–∞–ª—å—è–Ω—Å–∫–∏–π, –ü–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–π –∏ –¥—Ä.
        """)
    
    # –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    with gr.Accordion("üì• –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ", open=True):
        input_type = gr.Radio(
            choices=["File", "Text"], 
            label="–¢–∏–ø –≤—Ö–æ–¥–∞", 
            value="File"
        )
        
        file_input = gr.File(
            label="–ê—É–¥–∏–æ / –í–∏–¥–µ–æ / –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ",
            file_types=["audio", "video", "image"]
        )
        
        input_text = gr.Textbox(
            label="–í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç", 
            visible=False, 
            lines=5,
            placeholder="–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –∏ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏..."
        )
        
        media_type = gr.Dropdown(
            choices=["Audio", "Video", "Image"], 
            label="–¢–∏–ø –º–µ–¥–∏–∞", 
            visible=True,
            info="–ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ñ–∞–π–ª—É, –Ω–æ –º–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –≤—Ä—É—á–Ω—É—é"
        )

        youtube_input = gr.Textbox(label="YouTube URL (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)", placeholder="https://www.youtube.com/watch?v=...")

    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    with gr.Accordion("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏", open=True):
        with gr.Row():
            source_lang = gr.Textbox(
                label="–ò—Å—Ö–æ–¥–Ω—ã–π —è–∑—ã–∫", 
                value="auto",
                placeholder="auto, en, ru, fr, de, es, ja...",
                info="'auto' –¥–ª—è –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è. –î–ª—è OCR: en, ru, fr, de, es, ja, ch_sim"
            )
            target_lang = gr.Textbox(
                label="–¶–µ–ª–µ–≤–æ–π —è–∑—ã–∫", 
                value="ru",
                placeholder="en, ru, fr, de, es, ja...",
                info="–Ø–∑—ã–∫ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –∏ TTS"
            )
        
        with gr.Row():
            do_transcribe = gr.Checkbox(
                label="–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è / OCR", 
                value=True,
                info="–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –∏–ª–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏"
            )
            do_translate = gr.Checkbox(
                label="–ü–µ—Ä–µ–≤–æ–¥", 
                value=True,
                info="–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ü–µ–ª–µ–≤–æ–π —è–∑—ã–∫"
            )
            do_tts = gr.Checkbox(
                label="TTS (—Å–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏)", 
                value=True,
                info="–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ —Ä–µ—á—å"
            )
            do_diarization = gr.Checkbox(
                label="–î–∏–∞—Ä–∏–∑–∞—Ü–∏—è", 
                value=False,
                info="–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º (—Ç—Ä–µ–±—É–µ—Ç—Å—è HF —Ç–æ–∫–µ–Ω)"
            )
            use_vad = gr.Checkbox(label="VAD (—É–¥–∞–ª–∏—Ç—å —Ç–∏—à–∏–Ω—É)", value=False)
            use_uvr = gr.Checkbox(label="UVR (–æ—Ç–¥–µ–ª–∏—Ç—å –≤–æ–∫–∞–ª)", value=False)
            use_direct_translate = gr.Checkbox(label="–ü—Ä—è–º–æ–π –ø–µ—Ä–µ–≤–æ–¥ –≤ Whisper (to EN)", value=False)
            output_srt = gr.Checkbox(label="–í—ã–≤–æ–¥ SRT —Å—É–±—Ç–∏—Ç—Ä–æ–≤", value=True)
    
    # –ö–Ω–æ–ø–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    process_btn = gr.Button("üöÄ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å", variant="secondary", scale=2)
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    with gr.Accordion("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã", open=True):
        output_text = gr.Textbox(
            label="–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏",
            lines=15,
            max_lines=60,
            interactive=False,
            elem_classes=["result-textbox"]
        )
        
        with gr.Row():
            output_audio = gr.Audio(
                label="–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–µ—á—å", 
                type="filepath"
            )
            output_download = gr.File(
                label="–°–∫–∞—á–∞—Ç—å –∞—É–¥–∏–æ",
                file_types=[".wav", ".mp3"]
            )
            text_export = gr.File(
                label="–°–∫–∞—á–∞—Ç—å —Ç–µ–∫—Å—Ç",
                file_types=[".txt"]
            )

    srt_output=gr.File(label="–°–∫–∞—á–∞—Ç—å SRT")
    # ===== –û–ë–†–ê–ë–û–¢–ß–ò–ö–ò –°–û–ë–´–¢–ò–ô =====
    
    def update_visibility(inp_type):
        file_vis = inp_type == "File"
        text_vis = inp_type == "Text"
        return (
            gr.update(visible=file_vis),
            gr.update(visible=text_vis),
            gr.update(visible=file_vis)
        )
    
    def toggle_cloning(chk):
        return gr.update(visible=chk)
    
    input_type.change(
        update_visibility, 
        inputs=[input_type], 
        outputs=[file_input, input_text, media_type]
    )
    
    do_cloning.change(
        toggle_cloning,
        inputs=[do_cloning],
        outputs=[ref_audio_input]
    )
    
    load_models_btn.click(
        load_selected_models,
        inputs=[whisper_dropdown, tts_dropdown],
        outputs=[model_status]
    )
    
    process_btn.click(
        process_media,
        inputs=[
            input_type, file_input, input_text, media_type,
            source_lang, target_lang,
            do_transcribe, do_translate, do_tts,
            whisper_dropdown, tts_dropdown,
            translation_model_dropdown,
            do_diarization,
            hf_token_input,
            ref_audio_input,
            youtube_input,
            use_vad,
            use_uvr,
            use_direct_translate,
            output_srt
        ],
        outputs=[output_text, output_audio, output_download, text_export, srt_output]
    )

if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0", 
        server_port=7860, 
        share=False,
        favicon_path=None,
        show_error=True
    )