import gc
import logging
import os
import tempfile
import time
from datetime import datetime
from functools import lru_cache

import easyocr
import fasttext
import gradio as gr
import moviepy as mp
import torch
from faster_whisper import WhisperModel
from pyannote.audio import Pipeline
from transformers.pipelines import pipeline
from TTS.api import TTS

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==================== –ù–ê–°–¢–†–û–ô–ö–ê NLLB ====================
NLLB_LANG_MAP = {
    "en": "eng_Latn",
    "ru": "rus_Cyrl",
    "fr": "fra_Latn",
    "de": "deu_Latn",
    "es": "spa_Latn",
    "zh": "zho_Hans",
    "ja": "jpn_Jpan",
    "ar": "arb_Arab",
    "pt": "por_Latn",
    "it": "ita_Latn",
    "ko": "kor_Hang",
    "hi": "hin_Deva",
}

# ==================== –ù–ê–°–¢–†–û–ô–ö–ê –ü–£–¢–ï–ô ====================
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
MODELS_DIR = os.path.join(PROJECT_ROOT, "models")

for subdir in ["whisper", "tts", "translation", "ocr", "huggingface", "fasttext"]:
    os.makedirs(os.path.join(MODELS_DIR, subdir), exist_ok=True)

os.environ["HF_HUB_CACHE"] = os.path.join(MODELS_DIR, "huggingface")
os.environ["TRANSFORMERS_CACHE"] = os.path.join(MODELS_DIR, "huggingface")
os.environ["HF_HOME"] = os.path.join(MODELS_DIR, "huggingface")
os.environ["COQUI_TTS_CACHE"] = os.path.join(MODELS_DIR, "tts")

FASTTEXT_MODEL_PATH = os.path.join(MODELS_DIR, "fasttext", "lid.176.bin")

logger.info(f"–ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤: {MODELS_DIR}")

# –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
device = "cuda" if torch.cuda.is_available() else "cpu"
logger.info(f"Using device: {device}")

# –ó–∞–≥—Ä—É–∑–∫–∞ fastText –º–æ–¥–µ–ª–∏
fasttext_model = None
if not os.path.exists(FASTTEXT_MODEL_PATH):
    logger.info("–ú–æ–¥–µ–ª—å fastText lid.176.bin –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–∫–∞—á–∏–≤–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏...")
    import urllib.request
    url = "https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin"
    urllib.request.urlretrieve(url, FASTTEXT_MODEL_PATH)
    logger.info("fastText –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

try:
    fasttext_model = fasttext.load_model(FASTTEXT_MODEL_PATH)
    logger.info("fastText –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –¥–ª—è language detection.")
except Exception as e:
    logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ fastText –º–æ–¥–µ–ª–∏: {e}")
    fasttext_model = None

# –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
WHISPER_MODELS = {
    "large-v3": "large-v3",
    "large-v3-turbo": "large-v3-turbo",
    "medium": "medium",
    "small": "small",
    "distil-large-v3": "Systran/faster-whisper-distil-large-v3",
}

TTS_MODELS = {
    "your_tts (multilingual)": "tts_models/multilingual/multi-dataset/your_tts",
    "en/tacotron2-DDC": "tts_models/en/ljspeech/tacotron2-DDC",
    "ru/vits": "tts_models/ru/multi-dataset/vits",
    "en/vits": "tts_models/en/ljspeech/vits",
    "en/vits-persian": "tts_models/en/vctk/vits",  # –ï—â—ë –æ–¥–Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å
}

# –ö–∞—Ä—Ç–∞ —Å–ø–∏–∫–µ—Ä–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ —è–∑—ã–∫–æ–≤
TTS_SPEAKERS = {
    "your_tts (multilingual)": {
        "en": "LJSpeech",
        "ru": "Russian Female",
        "fr": "French Female", 
        "de": "German Female",
        "es": "Spanish Female",
        "it": "Italian Female",
        "pt": "Portuguese Female",
        "default": "LJSpeech"
    },
    "ru/vits": {
        "ru": "Russian Female",
        "default": "Russian Female"
    },
    "en/vits": {
        "en": "p225",  # speaker_id –∏–∑ VCTK
        "default": "p225"
    },
    "en/vits-persian": {
        "en": "p225",
        "default": "p225"
    }
}

TRANSLATION_MODELS = [
    "Helsinki-NLP/opus-mt-mul-en",
    "Helsinki-NLP/opus-mt-tc-big-mul-en",
    "facebook/nllb-200-distilled-600M",
    "facebook/nllb-200-distilled-1.3B",
]

TRANSLATION_MODELS_PIVOT = [
    "Helsinki-NLP/opus-mt-mul-en",
    "Helsinki-NLP/opus-mt-tc-big-mul-en",
]

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
whisper_model = None
tts_model = None
current_whisper_name = None
current_tts_name = None
model_status_text = "–ú–æ–¥–µ–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã"

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OCR —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º —Å–ø–∏—Å–∫–æ–º —è–∑—ã–∫–æ–≤
# –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–¥—ã —è–∑—ã–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Ç–æ—á–Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è easyocr
SUPPORTED_OCR_LANGUAGES = ['en', 'ru', 'fr', 'de', 'es', 'ja']  # –£–±—Ä–∞–ª–∏ 'zh' –∏–∑ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞
try:
    ocr_reader = easyocr.Reader(
        SUPPORTED_OCR_LANGUAGES,
        download_enabled=True,
        model_storage_directory=os.path.join(MODELS_DIR, "ocr"),
        user_network_directory=os.path.join(MODELS_DIR, "ocr"),
        gpu=device == "cuda"
    )
    logger.info(f"OCR –º–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è —è–∑—ã–∫–æ–≤: {SUPPORTED_OCR_LANGUAGES}")
except Exception as e:
    logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OCR: {e}")
    # –ü—Ä–æ–±—É–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å –∞–Ω–≥–ª–∏–π—Å–∫–∏–º
    try:
        ocr_reader = easyocr.Reader(
            ['en'],
            download_enabled=True,
            model_storage_directory=os.path.join(MODELS_DIR, "ocr"),
            user_network_directory=os.path.join(MODELS_DIR, "ocr"),
            gpu=device == "cuda"
        )
        logger.info("OCR –º–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞")
    except Exception as e2:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ OCR: {e2}")
        ocr_reader = None

@lru_cache(maxsize=32)
def get_translator(model_name):
    """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –º–æ–¥–µ–ª–µ–π –ø–µ—Ä–µ–≤–æ–¥–∞"""
    try:
        # –ü—Ä–æ—Å—Ç–æ –Ω–µ —É–∫–∞–∑—ã–≤–∞–µ–º cache_dir - –º–æ–¥–µ–ª–∏ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å—Å—è –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        # –∫–æ—Ç–æ—Ä—É—é –º—ã —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–∏–ª–∏ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
        return pipeline(
            "translation",
            model=model_name,
            device=device if device == "cuda" else -1
        )
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–≤–æ–¥–∞ {model_name}: {e}")
        raise

def log(msg):
    """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] {msg}")
    logger.info(msg)

def timed_step(step_name, func, *args, **kwargs):
    """–ò–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —à–∞–≥–∞"""
    start = time.time()
    log(f"–ù–∞—á–∞–ª–æ: {step_name}")
    try:
        result = func(*args, **kwargs)
        elapsed = time.time() - start
        log(f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ: {step_name} ‚Üí {elapsed:.2f} —Å–µ–∫")
        return result, elapsed
    except Exception as e:
        elapsed = time.time() - start
        log(f"–û—à–∏–±–∫–∞ –≤ {step_name}: {e} ‚Üí {elapsed:.2f} —Å–µ–∫")
        raise

def detect_language_fasttext(text):
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é fastText"""
    if not text.strip() or fasttext_model is None:
        return "unknown", 0.0
    try:
        prediction = fasttext_model.predict([text], k=1)
        lang_label = prediction[0][0][0]
        prob = prediction[1][0][0]
        lang_code = lang_label.replace('__label__', '')
        return lang_code, prob
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —è–∑—ã–∫–∞: {e}")
        return "unknown", 0.0

def load_whisper(model_key):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Whisper"""
    global whisper_model, current_whisper_name
    if current_whisper_name == model_key and whisper_model is not None:
        return f"Whisper —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω: {model_key}"
    
    try:
        model_id = WHISPER_MODELS[model_key]
        compute_type = "float16" if device == "cuda" else "int8"
        whisper_model = WhisperModel(
            model_id,
            device=device,
            compute_type=compute_type,
            download_root=os.path.join(MODELS_DIR, "whisper")
        )
        current_whisper_name = model_key
        return f"Whisper –∑–∞–≥—Ä—É–∂–µ–Ω: {model_key}"
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Whisper {model_key}: {e}")
        return f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Whisper: {str(e)}"

def load_tts(model_key):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ TTS"""
    global tts_model, current_tts_name
    if current_tts_name == model_key and tts_model is not None:
        return f"TTS —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω: {model_key}"
    
    try:
        model_name = TTS_MODELS[model_key]
        tts_model = TTS(model_name=model_name, progress_bar=True).to(device)
        current_tts_name = model_key
        return f"TTS –∑–∞–≥—Ä—É–∂–µ–Ω: {model_key}"
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ TTS {model_key}: {e}")
        return f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ TTS: {str(e)}"

def load_selected_models(whisper_model_key, tts_model_key):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    global model_status_text
    status = []
    
    try:
        if whisper_model_key:
            status.append(load_whisper(whisper_model_key))
        if tts_model_key:
            status.append(load_tts(tts_model_key))
        
        model_status_text = "\n".join(status) if status else "–ú–æ–¥–µ–ª–∏ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–ª–∏ –Ω–µ –≤—ã–±—Ä–∞–Ω—ã"
    except Exception as e:
        model_status_text = f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {str(e)}"
    
    return model_status_text

def cleanup_memory():
    """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏"""
    gc.collect()
    if device == "cuda":
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()

def extract_audio_from_video(video_path):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ –∏–∑ –≤–∏–¥–µ–æ"""
    try:
        video = mp.VideoFileClip(video_path)
        audio_path = tempfile.mktemp(suffix=".wav")
        video.audio.write_audiofile(audio_path, logger=None)
        video.close()
        return audio_path
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞—É–¥–∏–æ: {e}")
        raise

def transcribe_audio(audio_path, source_lang="auto"):
    """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞—É–¥–∏–æ —Å –ø–æ–º–æ—â—å—é Whisper"""
    if whisper_model is None:
        raise ValueError("Whisper –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")
    
    try:
        lang_param = None if source_lang.lower() == "auto" else source_lang
        segments, info = whisper_model.transcribe(
            audio_path, 
            language=lang_param, 
            beam_size=5, 
            vad_filter=True
        )
        full_text = " ".join([s.text for s in segments])
        return full_text, info.language, info.language_probability, list(segments)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: {e}")
        raise

def ocr_image(image_path, source_lang="en"):
    """–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏"""
    if ocr_reader is None:
        raise ValueError("OCR –º–æ–¥–µ–ª—å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    
    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∏–µ —è–∑—ã–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
        if source_lang == "auto":
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —è–∑—ã–∫–∏
            lang_list = SUPPORTED_OCR_LANGUAGES
        else:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–Ω—ã–π —è–∑—ã–∫
            if source_lang in SUPPORTED_OCR_LANGUAGES:
                lang_list = [source_lang]
            else:
                # –ï—Å–ª–∏ —è–∑—ã–∫ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–π
                lang_list = ['en']
                logger.warning(f"–Ø–∑—ã–∫ {source_lang} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è OCR. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞–Ω–≥–ª–∏–π—Å–∫–∏–π.")
        
        result = ocr_reader.readtext(
            image_path, 
            detail=0, 
            paragraph=True, 
            lang_list=lang_list
        )
        return " ".join(result)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ OCR: {e}")
        raise

def translate_text(text, source_lang, target_lang, model_name):
    """–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞"""
    if not text.strip():
        return ""
    
    try:
        # –ï—Å–ª–∏ —è–∑—ã–∫–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç
        if source_lang == target_lang:
            return text
        
        translator = get_translator(model_name)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º NLLB –µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω–∞ —ç—Ç–∞ –º–æ–¥–µ–ª—å
        if "nllb" in model_name.lower():
            src_code = NLLB_LANG_MAP.get(source_lang, f"{source_lang}_Latn")
            tgt_code = NLLB_LANG_MAP.get(target_lang, f"{target_lang}_Latn")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å —ç—Ç–∏ —è–∑—ã–∫–∏
            result = translator(text, src_lang=src_code, tgt_lang=tgt_code, max_length=1024)
            return result[0]['translation_text']
        else:
            # Pivot –ª–æ–≥–∏–∫–∞ —á–µ—Ä–µ–∑ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π
            if source_lang.lower() != "en":
                try:
                    # –ü—Ä–æ–±—É–µ–º –ø—Ä—è–º—É—é –º–æ–¥–µ–ª—å
                    direct_model = f"Helsinki-NLP/opus-mt-{source_lang}-en"
                    direct_translator = get_translator(direct_model)
                    en_text = direct_translator(text)[0]['translation_text']
                except:
                    # Fallback –Ω–∞ –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—É—é –º–æ–¥–µ–ª—å
                    en_text = translator(text, src_lang=source_lang)[0]['translation_text']
            else:
                en_text = text
            
            if target_lang.lower() == "en":
                return en_text
            
            # –ü–µ—Ä–µ–≤–æ–¥ —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —Ü–µ–ª–µ–≤–æ–π —è–∑—ã–∫
            try:
                tgt_model = f"Helsinki-NLP/opus-mt-en-{target_lang}"
                tgt_translator = get_translator(tgt_model)
                return tgt_translator(en_text)[0]['translation_text']
            except:
                # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç
                return en_text
                
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
        return f"[–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {str(e)}]"

def diarize_audio(audio_path, hf_token):
    """–î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ (—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º)"""
    if not hf_token or not audio_path:
        return []
    
    logger.info("–ó–∞–ø—É—Å–∫ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏...")
    try:
        pipe = Pipeline.from_pretrained(
            "pyannote/speaker-diarization-3.1", 
            use_auth_token=hf_token
        ).to(torch.device(device))
        
        diarization = pipe(audio_path)
        segments = []
        
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            segments.append({
                "start": turn.start, 
                "end": turn.end, 
                "speaker": speaker
            })
        
        return segments
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
        return []

def merge_transcription_and_diarization(whisper_segments, diar_segments):
    """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏"""
    if not diar_segments:
        return "\n".join([f"{s.text}" for s in whisper_segments])
    
    final = []
    for w_seg in whisper_segments:
        start, end = w_seg.start, w_seg.end
        speakers = {}
        
        for d in diar_segments:
            o_start = max(start, d["start"])
            o_end = min(end, d["end"])
            overlap = max(0, o_end - o_start)
            
            if overlap > 0:
                speakers[d["speaker"]] = speakers.get(d["speaker"], 0) + overlap
        
        best = max(speakers, key=speakers.get) if speakers else "Unknown"
        final.append(f"[{best}] ({start:.1f}-{end:.1f}): {w_seg.text}")
    
    return "\n".join(final)

def text_to_speech(text, target_lang="en", ref_audio=None, tts_model_name=None):
    """–°–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ —Å–ø–∏–∫–µ—Ä–æ–≤"""
    if tts_model is None or not text.strip():
        return None
    
    output_path = tempfile.mktemp(suffix=".wav")
    model_name = tts_model_name or current_tts_name or "unknown"
    
    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ø–∏–∫–µ—Ä–∞
        speaker = None
        if model_name in TTS_SPEAKERS:
            speaker = TTS_SPEAKERS[model_name].get(target_lang, TTS_SPEAKERS[model_name]["default"])
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∞—è –º–æ–¥–µ–ª—å TTS –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
        model_str = str(tts_model).lower()
        
        # –î–ª—è XTTS v2
        if "xtts" in model_str:
            if ref_audio and os.path.exists(ref_audio):
                tts_model.tts_to_file(
                    text=text, 
                    file_path=output_path, 
                    speaker_wav=ref_audio, 
                    language=target_lang
                )
            else:
                tts_model.tts_to_file(
                    text=text, 
                    file_path=output_path, 
                    language=target_lang
                )
        
        # –î–ª—è YourTTS
        elif "your_tts" in model_str and speaker:
            try:
                # –ü—Ä–æ–±—É–µ–º —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —Å–ø–∏–∫–µ—Ä–∞ –∏ —è–∑—ã–∫–∞
                tts_model.tts_to_file(
                    text=text, 
                    file_path=output_path,
                    # speaker=speaker,
                    language=target_lang
                )
            except Exception as e:
                # Fallback: –ø—Ä–æ–±—É–µ–º –±–µ–∑ —è–∑—ã–∫–∞
                logger.warning(f"YourTTS –æ—à–∏–±–∫–∞ —Å —è–∑—ã–∫–æ–º {target_lang}: {e}")
                tts_model.tts_to_file(
                    text=text, 
                    file_path=output_path,
                    speaker=speaker
                )
        
        # –î–ª—è VITS –º–æ–¥–µ–ª–µ–π
        elif "vits" in model_str and speaker:
            try:
                # VITS –º–æ–¥–µ–ª–∏ –æ–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç speaker_id
                tts_model.tts_to_file(
                    text=text, 
                    file_path=output_path,
                    speaker=speaker
                )
            except Exception as e:
                logger.warning(f"VITS –æ—à–∏–±–∫–∞ —Å–æ —Å–ø–∏–∫–µ—Ä–æ–º {speaker}: {e}")
                # –ü—Ä–æ–±—É–µ–º –±–µ–∑ —Å–ø–∏–∫–µ—Ä–∞
                tts_model.tts_to_file(text=text, file_path=output_path)
        
        # –î–ª—è –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π (tacotron –∏ —Ç.–¥.)
        else:
            try:
                # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Å —è–∑—ã–∫–æ–º
                tts_model.tts_to_file(
                    text=text, 
                    file_path=output_path, 
                    language=target_lang
                )
            except (TypeError, KeyError):
                # –ï—Å–ª–∏ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —è–∑—ã–∫, –ø—Ä–æ–±—É–µ–º –±–µ–∑
                try:
                    tts_model.tts_to_file(text=text, file_path=output_path)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –±–∞–∑–æ–≤–æ–≥–æ TTS: {e}")
                    # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ —Å speaker –µ—Å–ª–∏ –µ—Å—Ç—å
                    if speaker:
                        try:
                            tts_model.tts_to_file(
                                text=text, 
                                file_path=output_path,
                                speaker=speaker
                            )
                        except:
                            return None
                    else:
                        return None
        
        return output_path
        
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ TTS –¥–ª—è –º–æ–¥–µ–ª–∏ {model_name}: {e}")
        return None

def validate_inputs(input_type, file, input_text, media_type):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    errors = []
    
    if input_type == "File":
        if not file:
            errors.append("–§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω")
        elif media_type not in ["Audio", "Video", "Image"]:
            errors.append("–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø –º–µ–¥–∏–∞")
    elif input_type == "Text":
        if not input_text.strip():
            errors.append("–¢–µ–∫—Å—Ç –Ω–µ –≤–≤–µ–¥—ë–Ω")
    
    return errors

def process_media(
    input_type, file, input_text, media_type, source_lang, target_lang,
    do_transcribe, do_translate, do_tts, whisper_model_key, tts_model_key,
    translation_model, do_diarization=False, hf_token="", ref_audio=None
):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–µ–¥–∏–∞"""
    
    start_total = time.time()
    timings = []
    warnings = []
    result = ""
    
    try:
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        validation_errors = validate_inputs(input_type, file, input_text, media_type)
        if validation_errors:
            return "\n".join([f"‚ùå {e}" for e in validation_errors]), None, None
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–µ–π
        if do_transcribe and whisper_model is None:
            load_whisper(whisper_model_key)
        if do_tts and tts_model is None:
            load_tts(tts_model_key)
        
        if do_transcribe and whisper_model is None:
            warnings.append("‚ö†Ô∏è –ú–æ–¥–µ–ª—å Whisper –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        if do_tts and tts_model is None:
            warnings.append("‚ö†Ô∏è –ú–æ–¥–µ–ª—å TTS –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        
        if warnings:
            return "\n".join(warnings) + "\n\n" + model_status_text, None, None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        text = ""
        translated_text = ""
        tts_audio_path = None
        detected_lang = source_lang if source_lang != "auto" else "unknown"
        detected_prob = 0.0
        audio_path = None
        whisper_segments = []
        diar_segments = []
        
        log("‚îÄ" * 60)
        log(f"–û–±—Ä–∞–±–æ—Ç–∫–∞: {input_type} | {media_type} | transcribe={do_transcribe} translate={do_translate} tts={do_tts}")
        
        # –®–∞–≥ 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ –∏–∑ –≤–∏–¥–µ–æ
        if do_transcribe and input_type == "File" and media_type == "Video" and file:
            try:
                audio_path, t = timed_step("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ", extract_audio_from_video, file)
                timings.append(("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ", t))
            except Exception as e:
                return f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞—É–¥–∏–æ: {str(e)}", None, None
        
        # –®–∞–≥ 2: –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è / OCR
        if do_transcribe:
            if input_type != "File" or not file:
                text = "[–û—à–∏–±–∫–∞: –Ω—É–∂–µ–Ω —Ñ–∞–π–ª –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏]"
                timings.append(("–ù–µ—Ç —Ñ–∞–π–ª–∞ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏", 0))
            else:
                try:
                    if media_type in ["Audio", "Video"]:
                        audio_p = file if media_type == "Audio" else audio_path
                        if not audio_p or not os.path.exists(audio_p):
                            return f"‚ùå –ê—É–¥–∏–æ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_p}", None, None
                        
                        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
                        result_tuple = timed_step(
                            f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è ({media_type})", 
                            transcribe_audio, 
                            audio_p, 
                            source_lang
                        )
                        full_text, detected_lang, detected_prob, whisper_segments = result_tuple[0]
                        t_transcribe = result_tuple[1]
                        text = full_text
                        timings.append((f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è ({media_type})", t_transcribe))
                        
                        # –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞)
                        if do_diarization and hf_token and audio_p:
                            diar_segments, t_diar = timed_step(
                                "–î–∏–∞—Ä–∏–∑–∞—Ü–∏—è", 
                                diarize_audio, 
                                audio_p, 
                                hf_token
                            )
                            timings.append(("–î–∏–∞—Ä–∏–∑–∞—Ü–∏—è", t_diar))
                            
                            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–µ–π
                            if whisper_segments and diar_segments:
                                text = merge_transcription_and_diarization(whisper_segments, diar_segments)
                    
                    elif media_type == "Image":
                        # OCR
                        text_raw, t_ocr = timed_step(
                            "OCR", 
                            ocr_image, 
                            file, 
                            source_lang if source_lang != "auto" else "auto"
                        )
                        
                        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞
                        if fasttext_model and text_raw.strip():
                            detected_lang, detected_prob = timed_step(
                                "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞", 
                                detect_language_fasttext, 
                                text_raw
                            )[0]
                        
                        text = text_raw
                        timings.append(("OCR + –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞", t_ocr))
                        
                        if source_lang.lower() == "auto":
                            source_lang = detected_lang
                    
                    else:
                        text = "[–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø –º–µ–¥–∏–∞]"
                
                except Exception as e:
                    return f"‚ùå –û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏/OCR: {str(e)}", None, None
        
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –≤–≤–æ–¥
            text = input_text or ""
            timings.append(("–¢–µ–∫—Å—Ç –≤–∑—è—Ç –∏–∑ –ø–æ–ª—è", 0))
        
        # –®–∞–≥ 3: –ü–µ—Ä–µ–≤–æ–¥
        if do_translate and text.strip() and text != "[–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø –º–µ–¥–∏–∞]" and not text.startswith("[–û—à–∏–±–∫–∞"):
            try:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —è–∑—ã–∫ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞
                actual_source_lang = source_lang if source_lang != "auto" else detected_lang
                
                translated_text, t_translate = timed_step(
                    f"–ü–µ—Ä–µ–≤–æ–¥ ({actual_source_lang} ‚Üí {target_lang})",
                    translate_text,
                    text,
                    actual_source_lang,
                    target_lang,
                    translation_model
                )
                timings.append((f"–ü–µ—Ä–µ–≤–æ–¥ ({actual_source_lang} ‚Üí {target_lang})", t_translate))
            except Exception as e:
                translated_text = f"[–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {str(e)}]"
                timings.append(("–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞", 0))
        else:
            translated_text = text
        
        # –®–∞–≥ 4: TTS
        if do_tts and translated_text.strip() and not translated_text.startswith("[–û—à–∏–±–∫–∞"):
            try:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –¥–ª—è TTS
                tts_lang = target_lang if do_translate else detected_lang
                
                tts_audio_path, t_tts = timed_step(
                    f"TTS ({tts_lang})",
                    text_to_speech,
                    translated_text,
                    tts_lang,
                    ref_audio,
                    tts_model_key  # –ü–µ—Ä–µ–¥–∞—ë–º –∏–º—è –º–æ–¥–µ–ª–∏
                )
                timings.append(("–°–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏", t_tts))
            except Exception as e:
                tts_audio_path = None
                timings.append(("–û—à–∏–±–∫–∞ TTS", 0))
                log(f"–û—à–∏–±–∫–∞ TTS: {e}")
        
        # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        if audio_path and os.path.exists(audio_path):
            try:
                os.remove(audio_path)
            except:
                pass
        
        # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        cleanup_memory()
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        total_time = time.time() - start_total
        timings.append(("–û–±—â–µ–µ –≤—Ä–µ–º—è", total_time))
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        result = f"üìä **–°—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–µ–π:**\n{model_status_text}\n\n"
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
        if media_type in ["Audio", "Video"]:
            detector = "Whisper"
        elif media_type == "Image":
            detector = "fastText"
        else:
            detector = "manual"
        
        prob_str = f" (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å {detected_prob:.0%})" if detected_prob > 0 else ""
        
        if text and not text.startswith("[") and not text.startswith("‚ùå"):
            result += f"üìù **–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç** ({detected_lang}{prob_str}):\n{text[:1000]}{'...' if len(text) > 1000 else ''}\n\n"
        
        if do_translate and translated_text and translated_text != text:
            result += f"üåê **–ü–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç** ({target_lang}):\n{translated_text[:1000]}{' ...' if len(translated_text) > 1000 else ''}\n\n"
        
        # –¢–∞–±–ª–∏—Ü–∞ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        result += "```\n"
        result += "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n"
        result += "‚îÇ                  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è                    ‚îÇ\n"
        result += "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n"
        for step, sec in timings:
            result += f"‚îÇ {step:<35} ‚îÇ {sec:>10.2f} —Å–µ–∫ ‚îÇ\n"
        result += "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
        result += "```\n"
        
        log(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {total_time:.2f} —Å–µ–∫")
        log("‚îÄ" * 60)
        
        return result, tts_audio_path, tts_audio_path
    
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ process_media: {e}")
        return f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}", None, None


# ==================== GRADIO –ò–ù–¢–ï–†–§–ï–ô–° ====================

# CSS —Å—Ç–∏–ª–∏ –¥–ª—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
css = """
    .result-textbox textarea {
        min-height: 220px;
        max-height: 65vh;
        overflow-y: auto !important;
        resize: vertical;
        font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
        font-size: 14px;
    }
    .warning {
        color: #ff6b00;
        font-weight: bold;
    }
    .success {
        color: #00aa00;
        font-weight: bold;
    }
    .error {
        color: #ff0000;
        font-weight: bold;
    }
    .info-box {
        padding: 10px;
        border-radius: 5px;
        background: #f0f8ff;
        border-left: 4px solid #4a90e2;
        margin: 10px 0;
    }
"""

with gr.Blocks() as demo:
    
    gr.Markdown("# üéØ Local Media Processor")
    gr.Markdown("–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è, OCR, –ø–µ—Ä–µ–≤–æ–¥ –∏ —Å–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏ –≤ –æ–¥–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏")
    
    # –°–µ–∫—Ü–∏—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
    with gr.Accordion("üîê –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏", open=False):
        hf_token_input = gr.Textbox(
            label="HuggingFace Token (–¥–ª—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ pyannote)", 
            type="password",
            placeholder="hf_...",
            info="–¢–æ–∫–µ–Ω –Ω—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ –¥–ª—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏"
        )
    
    # –°–µ–∫—Ü–∏—è –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–µ–π
    with gr.Accordion("ü§ñ –í—ã–±–æ—Ä –º–æ–¥–µ–ª–µ–π", open=True):
        with gr.Row():
            whisper_dropdown = gr.Dropdown(
                choices=list(WHISPER_MODELS.keys()), 
                label="–ú–æ–¥–µ–ª—å Whisper", 
                value="large-v3",
                info="–î–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∞—É–¥–∏–æ/–≤–∏–¥–µ–æ"
            )
            tts_dropdown = gr.Dropdown(
                choices=list(TTS_MODELS.keys()), 
                label="–ú–æ–¥–µ–ª—å TTS", 
                value="your_tts (multilingual)",  # –ò–∑–º–µ–Ω–∏—Ç–µ –∑–¥–µ—Å—å
                info="–î–ª—è —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏"
            )
        
        translation_model_dropdown = gr.Dropdown(
            choices=TRANSLATION_MODELS,
            label="–ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–≤–æ–¥–∞",
            value="facebook/nllb-200-distilled-600M",
            info="NLLB –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –±–æ–ª—å—à–µ —è–∑—ã–∫–æ–≤"
        )
    
    # –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–æ–ª–æ—Å–∞
    with gr.Accordion("üé§ –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–æ–ª–æ—Å–∞", open=False):
        do_cloning = gr.Checkbox(
            label="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–æ–ª–æ—Å–∞", 
            value=False,
            info="–¢—Ä–µ–±—É–µ—Ç—Å—è —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–µ –∞—É–¥–∏–æ"
        )
        ref_audio_input = gr.Audio(
            label="–û–±—Ä–∞–∑–µ—Ü –≥–æ–ª–æ—Å–∞ (Reference Audio)", 
            type="filepath", 
            visible=False
        )
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
    load_models_btn = gr.Button("üîÑ –ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏", variant="primary")
    model_status = gr.Textbox(
        label="–°—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–µ–π", 
        interactive=False, 
        lines=3, 
        value=model_status_text
    )
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —è–∑—ã–∫–∞—Ö
    with gr.Accordion("‚ÑπÔ∏è –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —è–∑—ã–∫–∏", open=False):
        gr.Markdown("""
        ### –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è (Whisper):
        - –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ 100 —è–∑—ã–∫–æ–≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        
        ### OCR (EasyOCR):
        - –ê–Ω–≥–ª–∏–π—Å–∫–∏–π (en), –†—É—Å—Å–∫–∏–π (ru), –§—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π (fr)
        - –ù–µ–º–µ—Ü–∫–∏–π (de), –ò—Å–ø–∞–Ω—Å–∫–∏–π (es), –Ø–ø–æ–Ω—Å–∫–∏–π (ja)
        
        ### –ü–µ—Ä–µ–≤–æ–¥ (NLLB):
        - –ë–æ–ª–µ–µ 200 —è–∑—ã–∫–æ–≤
        
        ### TTS (XTTS v2):
        - –ê–Ω–≥–ª–∏–π—Å–∫–∏–π, –†—É—Å—Å–∫–∏–π, –§—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π, –ù–µ–º–µ—Ü–∫–∏–π
        - –ò—Å–ø–∞–Ω—Å–∫–∏–π, –ò—Ç–∞–ª—å—è–Ω—Å–∫–∏–π, –ü–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–π, –ü–æ–ª—å—Å–∫–∏–π
        - –¢—É—Ä–µ—Ü–∫–∏–π, –ì—Ä–µ—á–µ—Å–∫–∏–π, –ë–æ–ª–≥–∞—Ä—Å–∫–∏–π, –î–∞—Ç—Å–∫–∏–π
        - –§–∏–Ω—Å–∫–∏–π, –ì–æ–ª–ª–∞–Ω–¥—Å–∫–∏–π, –ß–µ—à—Å–∫–∏–π, –í–µ–Ω–≥–µ—Ä—Å–∫–∏–π
        - –†—É–º—ã–Ω—Å–∫–∏–π, –®–≤–µ–¥—Å–∫–∏–π
        """)
    
    # –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    with gr.Accordion("üì• –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ", open=True):
        input_type = gr.Radio(
            choices=["File", "Text"], 
            label="–¢–∏–ø –≤—Ö–æ–¥–∞", 
            value="File"
        )
        
        file_input = gr.File(
            label="–ê—É–¥–∏–æ / –í–∏–¥–µ–æ / –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ",
            file_types=["audio", "video", "image"]
        )
        
        input_text = gr.Textbox(
            label="–í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç", 
            visible=False, 
            lines=5,
            placeholder="–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –∏ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏..."
        )
        
        media_type = gr.Dropdown(
            choices=["Audio", "Video", "Image"], 
            label="–¢–∏–ø –º–µ–¥–∏–∞", 
            visible=True,
            info="–í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"
        )
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    with gr.Accordion("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏", open=True):
        with gr.Row():
            source_lang = gr.Textbox(
                label="–ò—Å—Ö–æ–¥–Ω—ã–π —è–∑—ã–∫", 
                value="auto",
                placeholder="auto, en, ru, fr, de, es, ja...",
                info="'auto' –¥–ª—è –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è. –î–ª—è OCR –¥–æ—Å—Ç—É–ø–Ω—ã: en, ru, fr, de, es, ja"
            )
            target_lang = gr.Textbox(
                label="–¶–µ–ª–µ–≤–æ–π —è–∑—ã–∫", 
                value="ru",
                placeholder="en, ru, fr, de, es, ja...",
                info="–Ø–∑—ã–∫ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –∏ TTS"
            )
        
        with gr.Row():
            do_transcribe = gr.Checkbox(
                label="–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è / OCR", 
                value=True,
                info="–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –∏–ª–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏"
            )
            do_translate = gr.Checkbox(
                label="–ü–µ—Ä–µ–≤–æ–¥", 
                value=True,
                info="–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ü–µ–ª–µ–≤–æ–π —è–∑—ã–∫"
            )
            do_tts = gr.Checkbox(
                label="TTS (—Å–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏)", 
                value=True,
                info="–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ —Ä–µ—á—å"
            )
            do_diarization = gr.Checkbox(
                label="–î–∏–∞—Ä–∏–∑–∞—Ü–∏—è", 
                value=False,
                info="–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º (—Ç—Ä–µ–±—É–µ—Ç—Å—è HF —Ç–æ–∫–µ–Ω)"
            )
    
    # –ö–Ω–æ–ø–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    process_btn = gr.Button("üöÄ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å", variant="secondary", scale=2)
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    with gr.Accordion("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã", open=True):
        output_text = gr.Textbox(
            label="–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏",
            lines=15,
            max_lines=60,
            interactive=False,
            buttons=["copy"],
            elem_classes=["result-textbox"]
        )
        
        with gr.Row():
            output_audio = gr.Audio(
                label="–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–µ—á—å", 
                type="filepath"
            )
            output_download = gr.File(
                label="–°–∫–∞—á–∞—Ç—å –∞—É–¥–∏–æ",
                file_types=[".wav", ".mp3"]
            )
    
    # ===== –û–ë–†–ê–ë–û–¢–ß–ò–ö–ò –°–û–ë–´–¢–ò–ô =====
    
    def update_visibility(inp_type):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–∏–¥–∏–º–æ—Å—Ç–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –≤–≤–æ–¥–∞"""
        file_vis = inp_type == "File"
        text_vis = inp_type == "Text"
        return (
            gr.update(visible=file_vis),
            gr.update(visible=text_vis),
            gr.update(visible=file_vis)
        )
    
    def toggle_cloning(chk):
        """–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –≤–∏–¥–∏–º–æ—Å—Ç–∏ –ø–æ–ª—è –¥–ª—è —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ –∞—É–¥–∏–æ"""
        return gr.update(visible=chk)
    
    # –ü–æ–¥–ø–∏—Å–∫–∞ –Ω–∞ —Å–æ–±—ã—Ç–∏—è
    input_type.change(
        update_visibility, 
        inputs=[input_type], 
        outputs=[file_input, input_text, media_type]
    )
    
    do_cloning.change(
        toggle_cloning,
        inputs=[do_cloning],
        outputs=[ref_audio_input]
    )
    
    load_models_btn.click(
        load_selected_models,
        inputs=[whisper_dropdown, tts_dropdown],
        outputs=[model_status]
    )
    
    process_btn.click(
        process_media,
        inputs=[
            input_type, file_input, input_text, media_type,
            source_lang, target_lang,
            do_transcribe, do_translate, do_tts,
            whisper_dropdown, tts_dropdown,
            translation_model_dropdown,
            do_diarization,
            hf_token_input,
            ref_audio_input
        ],
        outputs=[output_text, output_audio, output_download]
    )

if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0", 
        server_port=7860, 
        share=False,
        favicon_path=None,
        show_error=True,
        theme="soft",
        css=css
    )